

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tutorial: setting the hyperparameters &mdash; George 0.1.0 documentation</title>
  

  
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  
    <link rel="top" title="George 0.1.0 documentation" href="../../"/>
        <link rel="next" title="Kernels" href="../kernels/"/>
        <link rel="prev" title="Tutorial: model fitting with correlated noise" href="../model/"/> 

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="../../" class="fa fa-home"> George</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
        
            <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../quickstart/">Getting started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../quickstart/#installation">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart/#a-simple-example">A Simple Example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model/">Tutorial: model fitting with correlated noise</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../model/#a-simple-mean-model">A Simple Mean Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model/#simulated-dataset">Simulated Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model/#assuming-white-noise">Assuming White Noise</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model/#modeling-the-noise">Modeling the Noise</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model/#the-final-fit">The Final Fit</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="">Tutorial: setting the hyperparameters</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#optimization">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sampling-marginalization">Sampling &amp; Marginalization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../kernels/">Kernels</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../kernels/#basic-kernels">Basic Kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernels/#radial-kernels">Radial Kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernels/#periodic-kernels">Periodic Kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernels/#combining-kernels">Combining Kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernels/#implementing-new-kernels">Implementing New Kernels</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../solvers/">Solvers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../solvers/#basic-solver">Basic Solver</a></li>
<li class="toctree-l2"><a class="reference internal" href="../solvers/#hodlr-solver">HODLR Solver</a></li>
</ul>
</li>
</ul>

        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../">George</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../">Docs</a> &raquo;</li>
      
    <li>Tutorial: setting the hyperparameters</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="https://github.com/dfm/george/blob/master/docs/user/hyper.rst" class="fa fa-github"> Edit on GitHub</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            
  <span class="target" id="module-george"></span><div class="section" id="tutorial-setting-the-hyperparameters">
<span id="hyper"></span><h1>Tutorial: setting the hyperparameters<a class="headerlink" href="#tutorial-setting-the-hyperparameters" title="Permalink to this headline">Â¶</a></h1>
<p>In this demo, we&#8217;ll reproduce the analysis for Figure 5.6 in <a class="reference external" href="http://www.gaussianprocess.org/gpml/chapters/RW5.pdf">Chapter 5 of
Rasmussen &amp; Williams (R&amp;W)</a>.
The data are measurements of the atmospheric CO2 concentration made at Mauna
Loa, Hawaii (Keeling &amp; Whorf 2004).
The dataset is said to be available online but I couldn&#8217;t seem to download it
from the original source.
Luckily the <a class="reference external" href="http://statsmodels.sourceforge.net/">statsmodels</a> package
<a class="reference external" href="http://statsmodels.sourceforge.net/devel/datasets/generated/co2.html">includes a copy</a> that
we can load as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="kn">as</span> <span class="nn">sm</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">get_rdataset</span><span class="p">(</span><span class="s">&quot;co2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">data</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">time</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">co2</span><span class="p">)</span>
</pre></div>
</div>
<p>These data are plotted in the figure below:</p>
<img alt="../../_images/data.png" src="../../_images/data.png" />
<p>In this figure, you can see that there is periodic (or quasi-periodic) signal
with a year-long period superimposed on a long term trend.
We will follow R&amp;W and model these effects non-parametrically using a
complicated covariance function.
The covariance function that we&#8217;ll use is:</p>
<div class="math">
\[k(r) = k_1(r) + k_2(r) + k_3(r) + k_4(r)\]</div>
<p>where</p>
<div class="math">
\[\begin{split}\begin{eqnarray}
k_1(r) &amp;=&amp; \theta_1^2 \, \exp \left(-\frac{r^2}{2\,\theta_2} \right) \\
k_2(r) &amp;=&amp; \theta_3^2 \, \exp \left(-\frac{r^2}{2\,\theta_4}
                                     -\theta_5\,\sin^2\left(
                                     \frac{\pi\,r}{\theta_6}\right)
                                    \right) \\
k_3(r) &amp;=&amp; \theta_7^2 \, \left [ 1 + \frac{r^2}{2\,\theta_8\,\theta_9}
                         \right ]^{-\theta_8} \\
k_4(r) &amp;=&amp; \theta_{10}^2 \, \exp \left(-\frac{r^2}{2\,\theta_{11}} \right)
            + \theta_{12}^2\,\delta_{ij}
\end{eqnarray}\end{split}\]</div>
<p>We can implement this kernel in George as follows (we&#8217;ll use the R&amp;W results
as the hyperparameters for now):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">george</span> <span class="kn">import</span> <span class="n">kernels</span>

<span class="n">k1</span> <span class="o">=</span> <span class="mf">66.0</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">kernels</span><span class="o">.</span><span class="n">ExpSquaredKernel</span><span class="p">(</span><span class="mf">67.0</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">k2</span> <span class="o">=</span> <span class="mf">2.4</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">kernels</span><span class="o">.</span><span class="n">ExpSquaredKernel</span><span class="p">(</span><span class="mi">90</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">kernels</span><span class="o">.</span><span class="n">ExpSine2Kernel</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="mf">1.3</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">k3</span> <span class="o">=</span> <span class="mf">0.66</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">kernels</span><span class="o">.</span><span class="n">RationalQuadraticKernel</span><span class="p">(</span><span class="mf">0.78</span><span class="p">,</span> <span class="mf">1.2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">k4</span> <span class="o">=</span> <span class="mf">0.18</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">kernels</span><span class="o">.</span><span class="n">ExpSquaredKernel</span><span class="p">(</span><span class="mf">1.6</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">kernels</span><span class="o">.</span><span class="n">WhiteKernel</span><span class="p">(</span><span class="mf">0.19</span><span class="p">)</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="n">k1</span> <span class="o">+</span> <span class="n">k2</span> <span class="o">+</span> <span class="n">k3</span> <span class="o">+</span> <span class="n">k4</span>
</pre></div>
</div>
<div class="section" id="optimization">
<h2>Optimization<a class="headerlink" href="#optimization" title="Permalink to this headline">Â¶</a></h2>
<p>If we want to find the &#8220;best-fit&#8221; hyperparameters, we should <em>optimize</em> an
objective function.
The two standard functions (as described in Chapter 5 of R&amp;W) are the
marginalized ln-likelihood and the cross validation likelihood.
George implements the former in the <a class="reference internal" href="../solvers/#george.GP.lnlikelihood" title="george.GP.lnlikelihood"><tt class="xref py py-func docutils literal"><span class="pre">GP.lnlikelihood()</span></tt></a> function and the
gradient with respect to the hyperparameters in the
<a class="reference internal" href="../solvers/#george.GP.grad_lnlikelihood" title="george.GP.grad_lnlikelihood"><tt class="xref py py-func docutils literal"><span class="pre">GP.grad_lnlikelihood()</span></tt></a> function:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">george</span>
<span class="n">gp</span> <span class="o">=</span> <span class="n">george</span><span class="o">.</span><span class="n">GP</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="n">gp</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">gp</span><span class="o">.</span><span class="n">lnlikelihood</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">gp</span><span class="o">.</span><span class="n">grad_lnlikelihood</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<p>In general, you&#8217;ll probably want to write a custom routine for optimizing this
function because the optimal choice of method seems to be very problem
dependent but George does come with a simple gradient-based non-linear
optimization routine that we&#8217;ll use for this demo:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">p</span><span class="p">,</span> <span class="n">results</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>After running this optimization, we find a final ln-likelihood of -100.22
(slightly better than the result in R&amp;W) and the following parameter values:</p>
<table border="1" class="docutils">
<colgroup>
<col width="47%" />
<col width="27%" />
<col width="27%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">&nbsp;</th>
<th class="head">result</th>
<th class="head">R&amp;W</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><span class="math">\(\theta_{1}\)</span></td>
<td>66.10</td>
<td>66.00</td>
</tr>
<tr class="row-odd"><td><span class="math">\(\theta_{2}\)</span></td>
<td>4488.61</td>
<td>4489.00</td>
</tr>
<tr class="row-even"><td><span class="math">\(\theta_{3}\)</span></td>
<td>2.16</td>
<td>2.40</td>
</tr>
<tr class="row-odd"><td><span class="math">\(\theta_{4}\)</span></td>
<td>8100.55</td>
<td>8100.00</td>
</tr>
<tr class="row-even"><td><span class="math">\(\theta_{5}\)</span></td>
<td>0.91</td>
<td>1.18</td>
</tr>
<tr class="row-odd"><td><span class="math">\(\theta_{6}\)</span></td>
<td>1.00</td>
<td>1.00</td>
</tr>
<tr class="row-even"><td><span class="math">\(\theta_{7}\)</span></td>
<td>0.87</td>
<td>0.66</td>
</tr>
<tr class="row-odd"><td><span class="math">\(\theta_{8}\)</span></td>
<td>0.11</td>
<td>0.78</td>
</tr>
<tr class="row-even"><td><span class="math">\(\theta_{9}\)</span></td>
<td>0.20</td>
<td>1.44</td>
</tr>
<tr class="row-odd"><td><span class="math">\(\theta_{10}\)</span></td>
<td>0.11</td>
<td>0.18</td>
</tr>
<tr class="row-even"><td><span class="math">\(\theta_{11}\)</span></td>
<td>2.58</td>
<td>2.56</td>
</tr>
<tr class="row-odd"><td><span class="math">\(\theta_{12}\)</span></td>
<td>0.19</td>
<td>0.19</td>
</tr>
</tbody>
</table>
<p>We can plot our prediction of the CO2 concentration into the future using our
optimized Gaussian process model by running:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="mi">2025</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">cov</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov</span><span class="p">))</span>
</pre></div>
</div>
<p>and this gives a result just like Figure 5.6 from R&amp;W:</p>
<img alt="../../_images/figure.png" src="../../_images/figure.png" />
</div>
<div class="section" id="sampling-marginalization">
<h2>Sampling &amp; Marginalization<a class="headerlink" href="#sampling-marginalization" title="Permalink to this headline">Â¶</a></h2>
<p>The prediction made in the previous section take into account uncertainties
due to the fact that a Gaussian process is stochastic but it doesn&#8217;t take into
account any uncertainties in the <em>values of the hyperparameters</em>.
This won&#8217;t matter if the hyperparameters are very well constrained by the data
but in this case, many of the parameters are actually poorly constrained.
To take this effect into account, we can apply prior probability functions to
the hyperparameters and marginalize using <a class="reference external" href="http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov chain Monte Carlo (MCMC)</a>.
To do this, we&#8217;ll use the <a class="reference external" href="http://dan.iel.fm/emcee">emcee</a> package.</p>
<p>First, we define the probabilistic model:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">lnprob</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="c"># Trivial improper prior: uniform in the log.</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">((</span><span class="o">-</span><span class="mi">10</span> <span class="o">&gt;</span> <span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">p</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">)):</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">lnprior</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c"># Update the kernel and compute the lnlikelihood.</span>
    <span class="n">kernel</span><span class="o">.</span><span class="n">pars</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lnprior</span> <span class="o">+</span> <span class="n">gp</span><span class="o">.</span><span class="n">lnlikelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">quiet</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<p>In this function, we&#8217;ve applied a prior on every parameter that is uniform in
the natural log between -10 and 10.
The <tt class="docutils literal"><span class="pre">quiet</span></tt> argument in the call to <a class="reference internal" href="../solvers/#george.GP.lnlikelihood" title="george.GP.lnlikelihood"><tt class="xref py py-func docutils literal"><span class="pre">GP.lnlikelihood()</span></tt></a> means that that
function will return <tt class="docutils literal"><span class="pre">-numpy.inf</span></tt> if the kernel is invalid or if there are
any linear algebra errors (otherwise it would raise an exception).</p>
<p>Then, we run the sampler (this will probably take a while to run if you want
to repeat this analysis):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">emcee</span>

<span class="c"># You need to compute the GP once before starting. Then the sample list</span>
<span class="c"># will be saved.</span>
<span class="n">gp</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

<span class="c"># Set up the sampler.</span>
<span class="n">nwalkers</span><span class="p">,</span> <span class="n">ndim</span> <span class="o">=</span> <span class="mi">36</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">emcee</span><span class="o">.</span><span class="n">EnsembleSampler</span><span class="p">(</span><span class="n">nwalkers</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">lnprob</span><span class="p">)</span>

<span class="c"># Initialize the walkers.</span>
<span class="n">p0</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">kernel</span><span class="o">.</span><span class="n">pars</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nwalkers</span><span class="p">)]</span>

<span class="k">print</span><span class="p">(</span><span class="s">&quot;Running burn-in&quot;</span><span class="p">)</span>
<span class="n">p0</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">&quot;Running production chain&quot;</span><span class="p">)</span>
<span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
<p>After this run, you can plot 50 samples from the marginalized predictive
probability distribution:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pl</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="mi">2025</span><span class="p">,</span> <span class="mi">250</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
    <span class="c"># Choose a random walker and step.</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">chain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">chain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">gp</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">pars</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">n</span><span class="p">])</span>

    <span class="c"># Plot a single sample.</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gp</span><span class="o">.</span><span class="n">sample_conditional</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="s">&quot;k&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
<p>This should give you a figure similar to this one:</p>
<img alt="../../_images/mcmc.png" src="../../_images/mcmc.png" />
<p>Comparing this to the same figure in the previous section, you&#8217;ll notice that
the error bars on the prediction are now substantially larger than before.
This is because we are now considering all the predictions that are consistent
with the data, not just the &#8220;best&#8221; prediction.
In general, even though it requires much more computation, it is more
conservative (and honest) to take all these sources of uncertainty into
account.</p>
</div>
</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../kernels/" class="btn btn-neutral float-right" title="Kernels"/>Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../model/" class="btn btn-neutral" title="Tutorial: model fitting with correlated noise"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2013-2014 Dan Foreman-Mackey.
    </p>
  </div>

  <a href="https://github.com/snide/sphinx_rtd_theme">Sphinx theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>
</footer>
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'0.1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../_static/js/analytics.js"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>
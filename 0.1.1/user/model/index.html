

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tutorial: model fitting with correlated noise &mdash; George 0.1.1 documentation</title>
  

  
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  
    <link rel="top" title="George 0.1.1 documentation" href="../../"/>
        <link rel="next" title="Tutorial: setting the hyperparameters" href="../hyper/"/>
        <link rel="prev" title="Getting started" href="../quickstart/"/> 

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="../../" class="fa fa-home"> George</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
        
            <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../quickstart/">Getting started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../quickstart/#installation">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart/#a-simple-example">A Simple Example</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="">Tutorial: model fitting with correlated noise</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#a-simple-mean-model">A Simple Mean Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#simulated-dataset">Simulated Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="#assuming-white-noise">Assuming White Noise</a></li>
<li class="toctree-l2"><a class="reference internal" href="#modeling-the-noise">Modeling the Noise</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-final-fit">The Final Fit</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../hyper/">Tutorial: setting the hyperparameters</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../hyper/#optimization">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../hyper/#sampling-marginalization">Sampling &amp; Marginalization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../kernels/">Kernels</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../kernels/#basic-kernels">Basic Kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernels/#radial-kernels">Radial Kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernels/#periodic-kernels">Periodic Kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernels/#combining-kernels">Combining Kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernels/#implementing-new-kernels">Implementing New Kernels</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../solvers/">Solvers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../solvers/#basic-solver">Basic Solver</a></li>
<li class="toctree-l2"><a class="reference internal" href="../solvers/#hodlr-solver">HODLR Solver</a></li>
</ul>
</li>
</ul>

        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../">George</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../">Docs</a> &raquo;</li>
      
    <li>Tutorial: model fitting with correlated noise</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="https://github.com/dfm/george/blob/master/docs/user/model.rst" class="fa fa-github"> Edit on GitHub</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            
  <div class="section" id="tutorial-model-fitting-with-correlated-noise">
<span id="model"></span><h1>Tutorial: model fitting with correlated noise<a class="headerlink" href="#tutorial-model-fitting-with-correlated-noise" title="Permalink to this headline">¶</a></h1>
<p>In this example, we&#8217;re going to simulate a common data analysis situation
where our dataset exhibits unknown correlations in the noise.
When taking data, it is often possible to estimate the independent measurement
uncertainty on a single point (due to, for example, Poisson counting
statistics) but there are often residual systematics that correlate data
points.
The effect of this correlated noise can often be hard to estimate but ignoring
it can introduce substantial biases into your inferences.
In the following sections, we will consider a synthetic dataset with
correlated noise and a simple non-linear model.
We will start by fitting the model assuming that the noise is uncorrelated and
then improve on this model by modeling the covariance structure in the data
using a Gaussian process.</p>
<p>All the code used in this tutorial is available <a class="reference external" href="https://github.com/dfm/george/blob/master/docs/_code/model.py">here</a>.</p>
<div class="section" id="a-simple-mean-model">
<h2>A Simple Mean Model<a class="headerlink" href="#a-simple-mean-model" title="Permalink to this headline">¶</a></h2>
<p>The model that we&#8217;ll fit in this demo is a single Gaussian feature with three
parameters: amplitude <span class="math">\(\alpha\)</span>, location <span class="math">\(\ell\)</span>, and width
<span class="math">\(\sigma^2\)</span>.
I&#8217;ve chosen this model because is is the simplest non-linear model that I
could think of, and it is qualitatively similar to a few problems in astronomy
(fitting spectral features, measuring transit times, <em>etc.</em>).</p>
</div>
<div class="section" id="simulated-dataset">
<h2>Simulated Dataset<a class="headerlink" href="#simulated-dataset" title="Permalink to this headline">¶</a></h2>
<p>I simulated a dataset of 50 points with known correlated noise.
In fact, this example is somewhat artificial since the data <em>were</em> drawn from
a Gaussian process but in everything that follows, we&#8217;ll use a different
kernel function for our inferences in an attempt to make the situation
slightly more realistic.
A known white variance was also added to each data point and the resulting
dataset is:</p>
<img alt="../../_images/data1.png" src="../../_images/data1.png" />
<p>The true model parameters used to simulate this dataset are:</p>
<div class="math">
\[\alpha = -1\quad, \quad\quad
\ell = 0.1\quad, \quad\quad
\sigma^2 = 0.4\quad.\]</div>
</div>
<div class="section" id="assuming-white-noise">
<h2>Assuming White Noise<a class="headerlink" href="#assuming-white-noise" title="Permalink to this headline">¶</a></h2>
<p>Let&#8217;s start by doing the standard thing and assuming that the noise is
uncorrelated.
In this case, the ln-likelihood function of the data <span class="math">\(\{y_n\}\)</span> given the
parameters <span class="math">\(\theta\)</span> is</p>
<div class="math">
\[\ln p(\{y_n\}\,|\,\{t_n\},\,\{\sigma_n^2\},\,\theta) =
    -\frac{1}{2}\,\sum_{n=1}^N \frac{[y_n - f_\theta(t_n)]^2}{\sigma_n^2}
    + A\]</div>
<p>where <span class="math">\(A\)</span> doesn&#8217;t depend on <span class="math">\(\theta\)</span> so it is irrelevant for our
purposes and <span class="math">\(f_\theta(t)\)</span> is our model function.</p>
<p>It is clear that there is some sort of systematic trend in the data and we
don&#8217;t want to ignore that so we&#8217;ll simultaneously model a linear trend and the
Gaussian feature described in the previous section.
Therefore, our model is</p>
<div class="math">
\[f_\theta (t) = m\,t + b +
    \alpha\,\exp\left(-\frac{[t-\ell]^2}{2\,\sigma^2} \right)\]</div>
<p>where <span class="math">\(\theta\)</span> is the 5-dimensional parameter vector</p>
<div class="math">
\[\theta = \{ m,\,b,\,\alpha,\,\ell,\,\sigma^2 \} \quad.\]</div>
<p>The following code snippet is a simple implementation of this model in Python</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">model1</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">amp</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">sig2</span> <span class="o">=</span> <span class="n">params</span>
    <span class="k">return</span> <span class="n">m</span><span class="o">*</span><span class="n">t</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">amp</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="n">loc</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">sig2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">lnlike1</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">yerr</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(((</span><span class="n">y</span> <span class="o">-</span> <span class="n">model1</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span><span class="o">/</span><span class="n">yerr</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>To fit this model using MCMC (using <a class="reference external" href="http://dan.iel.fm/emcee">emcee</a>), we
need to first choose priors&#8212;in this case we&#8217;ll just use a simple uniform
prior on each parameter&#8212;and then combine these with our likelihood function
to compute the ln-probability (up to a normalization constant).
In code, this will be:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">lnprior1</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">amp</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">sig2</span> <span class="o">=</span> <span class="n">p</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">-</span><span class="mi">10</span> <span class="o">&lt;</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="mi">10</span> <span class="ow">and</span>  <span class="o">-</span><span class="mi">10</span> <span class="o">&lt;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="mi">10</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">10</span> <span class="o">&lt;</span> <span class="n">amp</span> <span class="o">&lt;</span> <span class="mi">10</span> <span class="ow">and</span>
            <span class="o">-</span><span class="mi">5</span> <span class="o">&lt;</span> <span class="n">loc</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">sig2</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">):</span>
        <span class="k">return</span> <span class="mf">0.0</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

<span class="k">def</span> <span class="nf">lnprob1</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">yerr</span><span class="p">):</span>
    <span class="n">lp</span> <span class="o">=</span> <span class="n">lnprior1</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lp</span> <span class="o">+</span> <span class="n">lnlike1</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">yerr</span><span class="p">)</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">lp</span><span class="p">)</span> <span class="k">else</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
</pre></div>
</div>
<p>Now that we have our model implemented, we&#8217;ll initialize the walkers and run
both a burn-in and production chain:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># We&#39;ll assume that the data are stored in a tuple:</span>
<span class="c">#    data = (t, y, yerr)</span>

<span class="kn">import</span> <span class="nn">emcee</span>

<span class="n">initial</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>
<span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">initial</span><span class="p">)</span>
<span class="n">p0</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">initial</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-8</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">nwalkers</span><span class="p">)]</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">emcee</span><span class="o">.</span><span class="n">EnsembleSampler</span><span class="p">(</span><span class="n">nwalkers</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">lnprob1</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">&quot;Running burn-in...&quot;</span><span class="p">)</span>
<span class="n">p0</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">sampler</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">&quot;Running production...&quot;</span><span class="p">)</span>
<span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
<p>After running the chain, we can plot the results using the <tt class="docutils literal"><span class="pre">flatchain</span></tt>
property of the sampler.
It is often useful to plot the results on top of the data as well.
To do this, we can over plot 24 posterior samples on top of the data:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pl</span>

<span class="c"># Plot the data.</span>
<span class="n">pl</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">yerr</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">&quot;.k&quot;</span><span class="p">,</span> <span class="n">capsize</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c"># The positions where the prediction should be computed.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="c"># Plot 24 posterior samples.</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">flatchain</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">24</span><span class="p">)]:</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">model1</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">&quot;#4682b4&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
<p>Running this code should make a figure like:</p>
<img alt="../../_images/ind-results.png" src="../../_images/ind-results.png" />
<p>In this figure, the data are shown as black points with error bars and the
posterior samples are shown as translucent blue lines.
These results seem, at face value, pretty satisfying.
But, since we know the true model parameters that were used to simulate the
data, we can assess our original assumption of uncorrelated noise.
To do this, we&#8217;ll plot all the projections of our posterior samples using
<a class="reference external" href="https://github.com/dfm/triangle.py">triangle.py</a> and over plot the true
values:</p>
<img alt="../../_images/ind-corner.png" src="../../_images/ind-corner.png" />
<p>In this figure, the blue lines are the true values used to simulate the data
and the black contours and histograms show the posterior constraints.
The constraints on the amplitude <span class="math">\(\alpha\)</span> and the width <span class="math">\(\sigma^2\)</span>
are consistent with the truth but the location of the feature <span class="math">\(\ell\)</span> is
<em>almost completely inconsistent with the truth!</em>
This would matter a lot if we were trying to precisely measure radial
velocities or transit times.</p>
</div>
<div class="section" id="modeling-the-noise">
<h2>Modeling the Noise<a class="headerlink" href="#modeling-the-noise" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">A full discussion of the theory of Gaussian processes is beyond the
scope of this demo&#8212;you should probably check out <a class="reference external" href="http://www.gaussianprocess.org/gpml/">Rasmussen &amp; Williams
(2006)</a>&#8212;but I&#8217;ll try to give a
quick qualitative motivation for our model.</p>
</div>
<p>In this section, instead of assuming that the noise is white, we&#8217;ll generalize
the likelihood function to include covariances between data points.
To do this, let&#8217;s start by re-writing the likelihood function from the
previous section as a matrix equation (if you squint, you&#8217;ll be able to work
out that we haven&#8217;t changed it at all):</p>
<div class="math">
\[\ln p(\{y_n\}\,|\,\{t_n\},\,\{\sigma_n^2\},\,\theta) =
    -\frac{1}{2}\,\boldsymbol{r}^\mathrm{T}\,K^{-1}\,\boldsymbol{r}
    -\frac{1}{2}\,\ln\det K - \frac{N}{2}\,\ln 2\pi\]</div>
<p>where</p>
<div class="math">
\[\begin{split}\boldsymbol{r} = \left ( \begin{array}{c}
    y_1 - f_\theta(t_1) \\
    y_2 - f_\theta(t_2) \\
    \vdots \\
    y_N - f_\theta(t_N) \\
\end{array}\right)\end{split}\]</div>
<p>is the residual vector and</p>
<div class="math">
\[\begin{split}K = \left ( \begin{array}{cccc}
    \sigma_1^2 &amp; 0 &amp; &amp; 0 \\
    0 &amp; \sigma_2^2 &amp; &amp; 0 \\
      &amp; &amp; \ddots &amp; \\
    0 &amp; 0 &amp; &amp; \sigma_N^2 \\
\end{array}\right)\end{split}\]</div>
<p>is the <span class="math">\(N \times N\)</span> data covariance matrix (where <span class="math">\(N\)</span> is the
number of data points).</p>
<p>The fact that <span class="math">\(K\)</span> is diagonal is the result of our earlier assumption
that the noise was white.
If we want to relax this assumption, we just need to start populating the
off-diagonal elements of this covariance matrix.
If we wanted to make every off-diagonal element of the matrix a free
parameter, there would be too many parameters to actually do any inference.
Instead, we can simply <em>model</em> the elements of this array as</p>
<div class="math">
\[K_{ij} = \sigma_i^2\,\delta_{ij} + k(t_i,\,t_j)\]</div>
<p>where <span class="math">\(\delta_{ij}\)</span> is the <a class="reference external" href="http://en.wikipedia.org/wiki/Kronecker_delta">Kronecker_delta</a> and <span class="math">\(k(\cdot,\,\cdot)\)</span>
is a covariance function that we get to choose.
<a class="reference external" href="http://www.gaussianprocess.org/gpml/chapters/RW4.pdf">Chapter 4</a> of
Rasmussen &amp; Williams discusses various choices for <span class="math">\(k\)</span> but for this
demo, we&#8217;ll just use the <a class="reference external" href="http://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function">Matérn-3/2 function</a>:</p>
<div class="math">
\[k(r) = a^2 \, \left( 1+\frac{\sqrt{3}\,r}{\tau} \right)\,
                \exp \left (-\frac{\sqrt{3}\,r}{\tau} \right )\]</div>
<p>where <span class="math">\(r = |t_i - t_j|\)</span>, and <span class="math">\(a^2\)</span> and <span class="math">\(\tau\)</span> are the
parameters of the model.</p>
</div>
<div class="section" id="the-final-fit">
<h2>The Final Fit<a class="headerlink" href="#the-final-fit" title="Permalink to this headline">¶</a></h2>
<p>Now we could go ahead and implement the ln-likelihood function that we came up
with in the previous section but that&#8217;s what George is for, after all!
To implement the model from the previous section using George, we can write
the following ln-likelihood function in Python:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">george</span>
<span class="kn">from</span> <span class="nn">george</span> <span class="kn">import</span> <span class="n">kernels</span>

<span class="k">def</span> <span class="nf">model2</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">amp</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">sig2</span> <span class="o">=</span> <span class="n">params</span>
    <span class="k">return</span> <span class="n">amp</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="n">loc</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">sig2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">lnlike2</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">yerr</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">tau</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">p</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">gp</span> <span class="o">=</span> <span class="n">george</span><span class="o">.</span><span class="n">GP</span><span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">kernels</span><span class="o">.</span><span class="n">Matern32Kernel</span><span class="p">(</span><span class="n">tau</span><span class="p">))</span>
    <span class="n">gp</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">yerr</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gp</span><span class="o">.</span><span class="n">lnlikelihood</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">model2</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">lnprior2</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="n">lna</span><span class="p">,</span> <span class="n">lntau</span><span class="p">,</span> <span class="n">amp</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">sig2</span> <span class="o">=</span> <span class="n">p</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">-</span><span class="mi">5</span> <span class="o">&lt;</span> <span class="n">lna</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="ow">and</span>  <span class="o">-</span><span class="mi">5</span> <span class="o">&lt;</span> <span class="n">lntau</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">10</span> <span class="o">&lt;</span> <span class="n">amp</span> <span class="o">&lt;</span> <span class="mi">10</span> <span class="ow">and</span>
            <span class="o">-</span><span class="mi">5</span> <span class="o">&lt;</span> <span class="n">loc</span> <span class="o">&lt;</span> <span class="mi">5</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">sig2</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">):</span>
        <span class="k">return</span> <span class="mf">0.0</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

<span class="k">def</span> <span class="nf">lnprob2</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">yerr</span><span class="p">):</span>
    <span class="n">lp</span> <span class="o">=</span> <span class="n">lnprior2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lp</span> <span class="o">+</span> <span class="n">lnlike2</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">yerr</span><span class="p">)</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">lp</span><span class="p">)</span> <span class="k">else</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
</pre></div>
</div>
<p>As before, let&#8217;s run MCMC on this model:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">initial</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>
<span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">initial</span><span class="p">)</span>
<span class="n">p0</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">initial</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-8</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">nwalkers</span><span class="p">)]</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">emcee</span><span class="o">.</span><span class="n">EnsembleSampler</span><span class="p">(</span><span class="n">nwalkers</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">lnprob2</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">&quot;Running first burn-in...&quot;</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">p0</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">lnp</span><span class="p">)]</span>
<span class="n">sampler</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="c"># Re-sample the walkers near the best walker from the previous burn-in.</span>
<span class="n">p0</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="o">+</span> <span class="mf">1e-8</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">nwalkers</span><span class="p">)]</span>
<span class="n">p0</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="mi">250</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">&quot;Running second burn-in...&quot;</span><span class="p">)</span>
<span class="n">p0</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="mi">250</span><span class="p">)</span>
<span class="n">sampler</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">&quot;Running production...&quot;</span><span class="p">)</span>
<span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
<p>You&#8217;ll notice that this time I&#8217;ve run two burn-in phases where each one is
half the length of the burn-in from the previous example.
Before the second burn-in, I re-sample the positions of the walkers in a tiny
ball around the position of the best walker in the previous run.
I found that this re-sampling step was useful because otherwise some of the
walkers started in a bad part of parameter space and took a while to converge
to something reasonable.</p>
<p>The plotting code for the results for this model is similar to the code in the
previous section.
First, we can plot the posterior samples on top of the data:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># Plot the data.</span>
<span class="n">pl</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">yerr</span><span class="o">=</span><span class="n">yerr</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">&quot;.k&quot;</span><span class="p">,</span> <span class="n">capsize</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c"># The positions where the prediction should be computed.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="c"># Plot 24 posterior samples.</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">flatchain</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">24</span><span class="p">)]:</span>
    <span class="c"># Set up the GP for this sample.</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">tau</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">gp</span> <span class="o">=</span> <span class="n">george</span><span class="o">.</span><span class="n">GP</span><span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">kernels</span><span class="o">.</span><span class="n">Matern32Kernel</span><span class="p">(</span><span class="n">tau</span><span class="p">))</span>
    <span class="n">gp</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">yerr</span><span class="p">)</span>

    <span class="c"># Compute the prediction conditioned on the observations and plot it.</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">sample_conditional</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">model2</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">model</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&quot;#4682b4&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
<p>This code should produce a figure like:</p>
<img alt="../../_images/gp-results.png" src="../../_images/gp-results.png" />
<p>The code for the corner plot is identical to the previous one.
Running that should give the following marginalized constraints:</p>
<img alt="../../_images/gp-corner.png" src="../../_images/gp-corner.png" />
<p>It is clear from this figure that the constraints obtained when modeling the
noise are less precise (the error bars are larger) but more accurate (less
biased).</p>
</div>
</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../hyper/" class="btn btn-neutral float-right" title="Tutorial: setting the hyperparameters"/>Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../quickstart/" class="btn btn-neutral" title="Getting started"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2013-2014 Dan Foreman-Mackey.
    </p>
  </div>

  <a href="https://github.com/snide/sphinx_rtd_theme">Sphinx theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>
</footer>
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'0.1.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script type="text/javascript" src="../../_static/js/analytics.js"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>